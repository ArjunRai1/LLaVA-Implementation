{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307,
     "referenced_widgets": [
      "f7608d76894a487d960b339eea0b50dd",
      "b2b02fc921a048448f498a16587a0eca",
      "c5c3ddfa6a244f5889ec3ffea8e34514",
      "ebd76e725c5a41f1bb590a21e868aeb7",
      "ad9a47060cc947258194a486b6134d81",
      "c9b1b559fcfa416fa98b1e0770a8ebb5",
      "21ec3ba8833f42cba7a94420b64032f7",
      "3651c71bb48f48dd8236d7fc2379ccc3",
      "2592a68283e8413fad162655898627b3",
      "6b03a875b239497ea98a33f2d35b930d",
      "fceb7798b32147fb8fceb33f04068931",
      "3ba35af18f89474d8bc5605103ad000a",
      "c65ecc0cf525482384a251755cf6d725",
      "ed7a3539a4074bc49f874ee1cbab1657",
      "35dd9c32ecd24a358e21bde04429756a",
      "edbd7b757d4b4e6db5ca3890f38a792b",
      "700e9899877f474cbc3c0a87939800f8",
      "ad322a4b98074908a8fae511fc5e4d39",
      "799087353057484086e965b24bf88619",
      "18e27f54b5ff4357aa7ac839fc3493a1",
      "9197e1f3d20749e58248316726af68cc",
      "a361f696d9504765aa287d84081cf002",
      "da247376d1b14677b9c9cac30055f607",
      "04588d9b7b0243f8948985afcac38440",
      "4fe3c00df74149339377275208201f10",
      "45fd0b12d38c490594918ac7a6751a32",
      "235d6cf8e9a24424b73d816bea340025",
      "4d8d3574a95a4711b8148a5316cb85f5",
      "d199cf7c3f9c4889b5ccbe93ee542b27",
      "5801811c980a4653b006cb68180d5e03",
      "79628c89aa1747c6b07623fbd49e6874",
      "58aaf7ae2bb649eab26181f8ae2b19d9",
      "933d9dbecc68486cbe2fa9170c1447e5"
     ]
    },
    "id": "_FJyKkD5d4eb",
    "outputId": "641a3836-3b19-41ad-a67a-5de380b6d1d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7608d76894a487d960b339eea0b50dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba35af18f89474d8bc5605103ad000a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat.json:   0%|          | 0.00/211M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da247376d1b14677b9c9cac30055f607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'image', 'conversations'],\n",
      "    num_rows: 595375\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('liuhaotian/LLaVA-CC3M-Pretrain-595K', data_files=\"chat.json\", split='train')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68,
     "referenced_widgets": [
      "b01ebca28cdf438981dc43b88180d444",
      "3861d35688be4519a5e1a9c5a78f4d14",
      "dd8ec6d69bf4494cb0ff46e70da73100",
      "3f0a7f4e47254b43801e01b4562022ab",
      "8e8c6eb0d7aa40c8837856c5eeefaf65",
      "6b83cead7446413daaac9480a25830d8",
      "f561ed7f4fbc4c008bb2f65805c6ffee",
      "7d5334df892a41879d79cd2ecd51e7ab",
      "d78ba84806354173bff052a3692f7ef4",
      "ccb57b2f32564a1ca24ecdc80f69b42f",
      "e0957e3da489450998dda279ab37b0bc"
     ]
    },
    "id": "BCCtqLSJf0G6",
    "outputId": "386a762e-a518-4512-ebfd-1e24447d80ea"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b01ebca28cdf438981dc43b88180d444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "images.zip:   0%|          | 0.00/6.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/root/.cache/huggingface/hub/datasets--liuhaotian--LLaVA-CC3M-Pretrain-595K/snapshots/814894e93db9e12a1dee78b9669e20e8606fd590/images.zip'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "zip_path = hf_hub_download(repo_id=\"liuhaotian/LLaVA-CC3M-Pretrain-595K\", filename=\"images.zip\", repo_type=\"dataset\")\n",
    "zip_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R2h18mQEhn4o",
    "outputId": "0e2bf0c0-b0c6-45a2-c74c-cca0f3e1ed62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCC_train_002582585.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((224, 224),\n",
       " {'from': 'gpt',\n",
       "  'value': 'olive oil is a healthy ingredient used liberally .'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile, io\n",
    "from PIL import Image\n",
    "zf = zipfile.ZipFile(zip_path)\n",
    "def load_image_from_zip(name):\n",
    "    with zf.open(name) as f:\n",
    "        return Image.open(io.BytesIO(f.read())).convert(\"RGB\")\n",
    "\n",
    "sample = ds[0]\n",
    "img_name = sample.get(\"image\") or (sample.get(\"id\") + \".jpg\")\n",
    "img = load_image_from_zip(img_name)\n",
    "print(img_name)\n",
    "img.size, sample[\"conversations\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "0845538263a2448495763a2304decfa8",
      "f93586ec05de41eb84f19685578280a6",
      "a0948f0b4bd945b78dd797d00179d50c",
      "3e0d2fcbf9954ab4beb25cc93f1f7d61",
      "ca4ff566d77549279e34c4e3c2fed1d7",
      "ebc299a97ecf4b4a86f75fcd6fd0413a",
      "b03090d8451c4570b01381c6f252f067",
      "3ac20b09403f4dcc90c8baaa610d8e12",
      "9f0ef28e3b5748948807de5d29b2ed05",
      "9808ef27803d45699d275bed7c403efe",
      "f0ac2141cfa542f7859c6a6e85b6d95d",
      "a21ca953bda94cd0b67b4e24fd730c15",
      "a239efd636f240c7a1a9171272a0d6d3",
      "fdbf4dc77a0a4ba8be72b3d54a3288ec",
      "76cb3719aae040a3a8894d705ff9eb07",
      "3d6f8cf76b014c8c9a4c1e70431787c8",
      "1cf4570bb0f640ba991e4fbb2f94d721",
      "6b19700b446948e4945b6931831dc41f",
      "c831322def9546e0ab51fa07917f20df",
      "040c8e23fe734c7d969bae2693959e5c",
      "6ae5c67a59cf4422af28afbc69f2800b",
      "3120fb20458a40dea48479aa75039203",
      "2042f0ba80074197aa597c162a593e03",
      "1290c12087b74cbcb460215674539293",
      "0daf27a0e30f404381334d082d277a22",
      "e8bd39c634004ae59a71616825890d02",
      "05a62fc5e181411d979ec3e9c9be8f03",
      "6cbcb4faab614cce852045cd663b127b",
      "c1273c8918c84e1fa10395f35d17c7f2",
      "16bc74eb2753420a9072f2a13f7db11d",
      "4c05a4e60a8a466eb47248cc0c1ac3f2",
      "e35519d497eb494a9ff4012be5008366",
      "0410d57decc04ee9a0f0b2dcd0d54635"
     ]
    },
    "id": "QfhReXrAh7mj",
    "outputId": "af3daba0-69b9-4e19-e1dc-45f88708cdc3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0845538263a2448495763a2304decfa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21ca953bda94cd0b67b4e24fd730c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2042f0ba80074197aa597c162a593e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import CLIPVisionModel, CLIPImageProcessor\n",
    "vision = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "for param in vision.parameters():\n",
    "  param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SMs2q-Hrnhnc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_vis_tokens(img):\n",
    "    inputs = processor(images=img, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        feats = vision(**inputs).last_hidden_state\n",
    "    return feats.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nKbT62TlLN4J"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, get_cosine_schedule_with_warmup,)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "llm = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "batch_size=2\n",
    "lr=2e-3\n",
    "epochs=2\n",
    "WARMUP_STEPS=100\n",
    "MAX_STEPS=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "irdj2RdeLRIQ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "class ChatZipDataset(Dataset):\n",
    "    def __init__(self, chat_json_path, images_zip_path, processor, tokenizer):\n",
    "        with open(chat_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.data = json.load(f)\n",
    "        self.zf = zipfile.ZipFile(images_zip_path, \"r\")\n",
    "        self.processor = processor\n",
    "        self.tok = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.data)\n",
    "\n",
    "    def _open_image(self, name):\n",
    "        with self.zf.open(name) as f:\n",
    "            return Image.open(io.BytesIO(f.read())).convert(\"RGB\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.data[idx]\n",
    "        img_name = rec.get(\"image\") or (rec.get(\"id\",\"\") + \".jpg\")\n",
    "        convs = rec.get(\"conversations\", [])\n",
    "        user_text = \"Describe the image.\"\n",
    "        asst_text = \"\"\n",
    "        for c in convs:\n",
    "            if c.get(\"from\",\"\").lower() == \"human\" and c.get(\"value\"):\n",
    "                user_text = c[\"value\"].replace(\"<image>\", \"\").strip() or \"Describe the image\"\n",
    "                break\n",
    "        for c in convs:\n",
    "            if c.get(\"from\",\"\").lower() == \"gpt\" and c.get(\"value\"):\n",
    "                asst_text = c[\"value\"].strip()\n",
    "                break\n",
    "\n",
    "        img = self._open_image(img_name)\n",
    "        vis_inputs = self.processor(images=img, return_tensors=\"pt\")\n",
    "\n",
    "        user_ids = self.tok(user_text, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)\n",
    "        asst_ids = self.tok(asst_text, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)\n",
    "        return {\"vis_inputs\": vis_inputs, \"user_ids\": user_ids, \"asst_ids\": asst_ids}\n",
    "\n",
    "class Collator:\n",
    "    def __init__(self, pad_id): self.pad_id = pad_id\n",
    "    def __call__(self, batch):\n",
    "        max_u = max(x[\"user_ids\"].size(0) for x in batch)\n",
    "        max_a = max(x[\"asst_ids\"].size(0) for x in batch)\n",
    "        user_ids, asst_ids, pv_list = [], [], []\n",
    "\n",
    "        for x in batch:\n",
    "            u, a = x[\"user_ids\"], x[\"asst_ids\"]\n",
    "            if u.size(0) < max_u:\n",
    "                u = torch.cat([u, torch.full((max_u - u.size(0),), self.pad_id, dtype=torch.long)], dim=0)\n",
    "            if a.size(0) < max_a:\n",
    "                a = torch.cat([a, torch.full((max_a - a.size(0),), self.pad_id, dtype=torch.long)], dim=0)\n",
    "            user_ids.append(u)\n",
    "            asst_ids.append(a)\n",
    "            pv_list.append(x[\"vis_inputs\"][\"pixel_values\"])\n",
    "        user_ids = torch.stack(user_ids, 0)\n",
    "        asst_ids = torch.stack(asst_ids, 0)\n",
    "        pixel_values = torch.cat(pv_list, dim=0)\n",
    "\n",
    "        return {\"user_ids\": user_ids, \"asst_ids\": asst_ids, \"pixel_values\": pixel_values}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TbU96SqfLncT"
   },
   "outputs": [],
   "source": [
    "class Stage1Model(nn.Module):\n",
    "    def __init__(self, vision, llm):\n",
    "        super().__init__()\n",
    "        self.vision = vision\n",
    "        for p in self.vision.parameters():\n",
    "          p.requires_grad = False\n",
    "        self.vision.eval()\n",
    "\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(llm)\n",
    "        for p in self.llm.parameters():\n",
    "          p.requires_grad = False\n",
    "        self.llm.eval()\n",
    "\n",
    "        v_dim   = self.vision.config.hidden_size\n",
    "        d_model = self.llm.config.hidden_size\n",
    "        self.projector = nn.Linear(v_dim, d_model)\n",
    "        self.tok_emb   = self.llm.get_input_embeddings()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_image(self, vis_inputs):\n",
    "        return self.vision(**vis_inputs).last_hidden_state\n",
    "\n",
    "    def forward(self, vis_inputs, user_ids, asst_ids):\n",
    "        with torch.no_grad():\n",
    "            v_tokens = self.encode_image(vis_inputs)\n",
    "        V = self.projector(v_tokens)\n",
    "\n",
    "        text_ids = torch.cat([user_ids, asst_ids], dim=1)\n",
    "        text_emb = self.tok_emb(text_ids)\n",
    "\n",
    "        inputs_embeds = torch.cat([V, text_emb], dim=1)\n",
    "        attn_mask = torch.ones(inputs_embeds.size()[:-1], dtype=torch.long, device=inputs_embeds.device)\n",
    "\n",
    "        B = user_ids.size(0); Tv = V.size(1); U = user_ids.size(1); A = asst_ids.size(1)\n",
    "        labels = torch.cat([\n",
    "            torch.full((B, Tv), -100, dtype=torch.long, device=inputs_embeds.device),\n",
    "            torch.full((B, U),  -100, dtype=torch.long, device=inputs_embeds.device),\n",
    "            asst_ids\n",
    "        ], dim=1)\n",
    "\n",
    "        out = self.llm(inputs_embeds=inputs_embeds, attention_mask=attn_mask, labels=labels)\n",
    "        return out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MgN4696oL4Rz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def train_stage1(chat_json_path, images_zip_path, max_steps=200):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm)\n",
    "    processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    ds = ChatZipDataset(chat_json_path, images_zip_path, processor, tokenizer)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, collate_fn=Collator(tokenizer.pad_token_id or 0))\n",
    "\n",
    "    model = Stage1Model(vision, llm).to(device)\n",
    "    opt = torch.optim.AdamW(model.projector.parameters(), lr=lr)\n",
    "    total_steps = min(max_steps, epochs * max(1, len(dl)))\n",
    "    sched = get_cosine_schedule_with_warmup(opt, WARMUP_STEPS, total_steps)\n",
    "\n",
    "    step = 0\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dl:\n",
    "            vis_inputs = {\"pixel_values\": batch[\"pixel_values\"].to(device)}\n",
    "            user_ids = batch[\"user_ids\"].to(device)\n",
    "            asst_ids = batch[\"asst_ids\"].to(device)\n",
    "\n",
    "            loss = model(vis_inputs=vis_inputs, user_ids=user_ids, asst_ids=asst_ids)\n",
    "            opt.zero_grad(); loss.backward(); opt.step(); sched.step()\n",
    "\n",
    "            step += 1\n",
    "            if step % 10 == 0:\n",
    "                print(f\"step {step}/{total_steps}  loss={loss.item():.4f}\")\n",
    "            if step >= total_steps: break\n",
    "        if step >= total_steps: break\n",
    "\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    torch.save(model.projector.state_dict(), \"checkpoints/projector_stage1.pt\")\n",
    "    print(\"Saved to checkpoints/projector_stage1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605,
     "referenced_widgets": [
      "78baf069f7644db48f070eb88b6b7166",
      "b45ae66038ed401aa45c646b62028068",
      "58ca6f98feba4da1939de6f4ac60acab",
      "4d1cfb4de5204b9dba8f0ce98113216d",
      "bb5de327b34a4a459fbff2f1a7490ce2",
      "6d3e72682d1c41aabfd007f158750652",
      "c4d05412a5cb46f49c3b2ecd93f5e301",
      "048de61def554b97ae1af2d85b1ed3ad",
      "abcc8d561dc643cbb72b61c69328c2a8",
      "8ae2cfbd9cd244e5859825d7ccab7ccd",
      "e00b1b5a940c449a83484ff10a575abe",
      "e8de09e24f204ae3baa21aee69c5f13a",
      "56ab050365a64de28ede61e8b9cc303a",
      "8b7f75ced29d4c1b887ae10896226263",
      "20682a42cba74406876cfc6f0b421d97",
      "fe0d2ad1eb464b5f8514231ffff5fbdd",
      "e34d9ba36bfb4f8caa1c97f532278993",
      "6446f38aea1f44e0b4171a04d7e0c583",
      "27920b232f6c471681d8964c7915fffb",
      "5215983f92c64918a3988155378da1d6",
      "356c9a4f436c46a6bef664892a85328a",
      "8878402968e845b3881678bad93e4d3d",
      "dd78698b26d548f49c45bf9a5b1bf33d",
      "9489e149a8da430084cdac45bd119152",
      "04b18bb4264e4eb1b970fef6a3d3f5f9",
      "58bb0d59c7ef49829f2025aef9c96267",
      "d146c6bbda4a4f66b8a2d48acf6da9e0",
      "734c00f6904d4c36bf52b76692b1795a",
      "59fe8a8169594a3580f41eced2be37f0",
      "393047d78fd2479790931e3c5979423d",
      "44c0ccef9f074bd2a69c274cbb51b713",
      "6c02d88ddc124587bd058ea771f3959d",
      "b64db9c9b03b4e27a227ee5a11929ef9",
      "20d2bcca990b4e279d08dd743979e7a5",
      "bcaf9d629f554e08a078fe8622828072",
      "9e51d70870a0401fb4d7b4d8bbb31c94",
      "8f816a07863d4add8353d75923db058e",
      "219288da75454710b49d534ca326c1c3",
      "a1e318a9f1ce4c269ca53274926c434c",
      "f69308ce514d42a0bcf967112ce785e9",
      "9fbc3bdae6c54999a0b43376673d4c2f",
      "0c4f6445decb4f1dafdae30a61074a9f",
      "48a02f71138142c5a597f95af70edf65",
      "749472f297714f578c99ba0d405a23a9",
      "8b9c9d5a53d043cbadd53f5f382c1af6",
      "a447b4c8421747d9a9f719356e5cbd9b",
      "9ef9740af467400da9c5c50dc1d703f5",
      "2fae989b2dac4d4a9002997f2c56a9dc",
      "1f58c81b3c3b410da79772d776744d7c",
      "d8577fc436d74f50ab20db081a90b044",
      "31917d3cdbdd4bc2b030982c46d6accb",
      "a79fe85864da4bd9b996f8238fa97c4a",
      "c26925203c184b5aa73535cdfcfbdfe7",
      "04b52681eb964b4497db4db7259451cc",
      "80ac2b76c32649c890808cdb561b601b",
      "cc10b6a7c21a49af8167a9eeac67f429",
      "e8adc3d318bd45ef9296c78b64b824cc",
      "a3c5ab4783794955b34d0954f7782b26",
      "d324a1fc27cf4939972e1b3c52253784",
      "bc66a4edb8ab4c44bf31ecf3f419b09a",
      "64b8eacbceac4515825bd0a21cde020f",
      "f65a3fd875c24ca9b2e3f16d99ca2448",
      "67eac2f794a84ca2977746637f1ba3df",
      "d13e847c0d1f43649c33c8706243adb2",
      "ce8f665f5a7c4e709a9b92911d963024",
      "8658d43edb3448948a34067f3bd55f34",
      "c2b76c94d4c34acfb656eb76b0bd710b",
      "ab91fe39d76349e89bdd1407cf42146a",
      "a10be8e4225f4b42a2a72fb17e01eb11",
      "67569168229b494bb7f28995691b809b",
      "2b1fa474e24b49ac95f28a7476d32cdb",
      "30ae6b8c43264a42b29399f38718b854",
      "f7ca47bcbd0143289c68380effbe1d68",
      "5d11b2734ac34c8894f64f96fa867a82",
      "c5ce5aefecd44112bc3e5a569909811e",
      "a1a1729c673148c2b0f48353379d7147",
      "6fe18ec32c9a4365a6081429c1f7fd1d"
     ]
    },
    "id": "irgT4EHoMVXS",
    "outputId": "b2236733-d6ed-41e8-d4f2-64436e9255ed"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78baf069f7644db48f070eb88b6b7166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8de09e24f204ae3baa21aee69c5f13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd78698b26d548f49c45bf9a5b1bf33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d2bcca990b4e279d08dd743979e7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9c9d5a53d043cbadd53f5f382c1af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc10b6a7c21a49af8167a9eeac67f429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b76c94d4c34acfb656eb76b0bd710b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10/200  loss=8.0888\n",
      "step 20/200  loss=6.5507\n",
      "step 30/200  loss=5.3876\n",
      "step 40/200  loss=7.3611\n",
      "step 50/200  loss=5.1082\n",
      "step 60/200  loss=4.1272\n",
      "step 70/200  loss=2.3804\n",
      "step 80/200  loss=2.5535\n",
      "step 90/200  loss=3.4138\n",
      "step 100/200  loss=3.9934\n",
      "step 110/200  loss=3.8943\n",
      "step 120/200  loss=4.4690\n",
      "step 130/200  loss=4.2606\n",
      "step 140/200  loss=3.6347\n",
      "step 150/200  loss=4.9465\n",
      "step 160/200  loss=3.6728\n",
      "step 170/200  loss=3.6709\n",
      "step 180/200  loss=3.8007\n",
      "step 190/200  loss=3.0642\n",
      "step 200/200  loss=4.0941\n",
      "Saved to checkpoints/projector_stage1.pt\n"
     ]
    }
   ],
   "source": [
    "chat_path = hf_hub_download(repo_id='liuhaotian/LLaVA-CC3M-Pretrain-595K', filename='chat.json', repo_type=\"dataset\")\n",
    "train_stage1(chat_path, zip_path, max_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Vb0-UE9TMsSK"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "class Stage2Model(nn.Module):\n",
    "    def __init__(self, vision, llm, projector_path):\n",
    "        super().__init__()\n",
    "        self.vision = CLIPVisionModel.from_pretrained(vision)\n",
    "        for p in self.vision.parameters():\n",
    "          p.requires_grad = False\n",
    "        self.vision.eval()\n",
    "        base_llm = AutoModelForCausalLM.from_pretrained(llm)\n",
    "        lora_cfg = LoraConfig(\n",
    "            r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "            target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "            bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        self.llm = get_peft_model(base_llm, lora_cfg)\n",
    "\n",
    "        v_dim = self.vision.config.hidden_size\n",
    "        d_model = self.llm.base_model.config.hidden_size\n",
    "        self.projector = nn.Linear(v_dim, d_model)\n",
    "\n",
    "        self.projector.load_state_dict(torch.load(projector_path, map_location=\"cpu\"))\n",
    "\n",
    "        self.tok_emb = self.llm.get_input_embeddings()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_image(self, vis_inputs):\n",
    "        return self.vision(**vis_inputs).last_hidden_state\n",
    "\n",
    "    def forward(self, vis_inputs, user_ids, asst_ids):\n",
    "        with torch.no_grad():\n",
    "            v_tokens = self.encode_image(vis_inputs)\n",
    "        V = self.projector(v_tokens)\n",
    "\n",
    "        text_ids = torch.cat([user_ids, asst_ids], dim=1)\n",
    "        text_emb = self.tok_emb(text_ids)\n",
    "\n",
    "        inputs_embeds = torch.cat([V, text_emb], dim=1)\n",
    "        attn_mask = torch.ones(inputs_embeds.size()[:-1], dtype=torch.long, device=inputs_embeds.device)\n",
    "\n",
    "        B = user_ids.size(0); Tv = V.size(1); U = user_ids.size(1); A = asst_ids.size(1)\n",
    "        labels = torch.cat([\n",
    "            torch.full((B, Tv), -100, dtype=torch.long, device=inputs_embeds.device),\n",
    "            torch.full((B, U),  -100, dtype=torch.long, device=inputs_embeds.device),\n",
    "            asst_ids\n",
    "        ], dim=1)\n",
    "\n",
    "        out = self.llm(inputs_embeds=inputs_embeds, attention_mask=attn_mask, labels=labels)\n",
    "        return out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JPnbCzXJo5h5"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE=2\n",
    "LR=2e-5\n",
    "EPOCHS=1\n",
    "WARMUP_STEPS=50\n",
    "MAX_STEPS=300\n",
    "\n",
    "def train_stage2(chat_json_path, images_zip_path, projector_ckpt=\"checkpoints/projector_stage1.pt\", max_steps=300):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm)\n",
    "    processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    ds = ChatZipDataset(chat_json_path, images_zip_path, processor, tokenizer)\n",
    "    dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=Collator(tokenizer.pad_token_id or 0))\n",
    "\n",
    "    model = Stage2Model(\"openai/clip-vit-large-patch14\", llm, projector_ckpt).to(device)\n",
    "\n",
    "    params = list(model.projector.parameters()) + [p for p in model.llm.parameters() if p.requires_grad]\n",
    "    opt = torch.optim.AdamW(params, lr=LR)\n",
    "\n",
    "    total_steps = min(max_steps, EPOCHS * max(1, len(dl)))\n",
    "    sched = get_cosine_schedule_with_warmup(opt, WARMUP_STEPS, total_steps)\n",
    "\n",
    "    step = 0\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for batch in dl:\n",
    "            vis_inputs = {\"pixel_values\": batch[\"pixel_values\"].to(device)}\n",
    "            user_ids = batch[\"user_ids\"].to(device)\n",
    "            asst_ids = batch[\"asst_ids\"].to(device)\n",
    "\n",
    "            loss = model(vis_inputs=vis_inputs, user_ids=user_ids, asst_ids=asst_ids)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            sched.step()\n",
    "\n",
    "            step += 1\n",
    "            if step % 10 == 0:\n",
    "                print(f\"[Stage-2] step {step}/{total_steps}  loss={loss.item():.4f}\")\n",
    "            if step >= total_steps: break\n",
    "        if step >= total_steps: break\n",
    "\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    torch.save(model.projector.state_dict(), \"checkpoints/projector_stage2.pt\")\n",
    "    model.llm.save_pretrained(\"checkpoints/llm_lora\")\n",
    "    print(\"Saved projector: checkpoints/projector_stage2.pt\")\n",
    "    print(\"Saved LoRA adapter: checkpoints/llm_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cbld3S2QpHun",
    "outputId": "0eda7367-3d6b-4f8b-d0e6-8f3bea89b5e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage-2] step 10/300  loss=4.2345\n",
      "[Stage-2] step 20/300  loss=4.2142\n",
      "[Stage-2] step 30/300  loss=3.4410\n",
      "[Stage-2] step 40/300  loss=3.1668\n",
      "[Stage-2] step 50/300  loss=2.8178\n",
      "[Stage-2] step 60/300  loss=5.5109\n",
      "[Stage-2] step 70/300  loss=3.1644\n",
      "[Stage-2] step 80/300  loss=3.5968\n",
      "[Stage-2] step 90/300  loss=2.6456\n",
      "[Stage-2] step 100/300  loss=4.3146\n",
      "[Stage-2] step 110/300  loss=2.7245\n",
      "[Stage-2] step 120/300  loss=1.7584\n",
      "[Stage-2] step 130/300  loss=3.2866\n",
      "[Stage-2] step 140/300  loss=3.4748\n",
      "[Stage-2] step 150/300  loss=3.8017\n",
      "[Stage-2] step 160/300  loss=2.9234\n",
      "[Stage-2] step 170/300  loss=2.1767\n",
      "[Stage-2] step 180/300  loss=2.7793\n",
      "[Stage-2] step 190/300  loss=3.6375\n",
      "[Stage-2] step 200/300  loss=2.5224\n",
      "[Stage-2] step 210/300  loss=3.2630\n",
      "[Stage-2] step 220/300  loss=2.6615\n",
      "[Stage-2] step 230/300  loss=2.6836\n",
      "[Stage-2] step 240/300  loss=2.9017\n",
      "[Stage-2] step 250/300  loss=2.4858\n",
      "[Stage-2] step 260/300  loss=2.8795\n",
      "[Stage-2] step 270/300  loss=2.9193\n",
      "[Stage-2] step 280/300  loss=2.8218\n",
      "[Stage-2] step 290/300  loss=3.3808\n",
      "[Stage-2] step 300/300  loss=2.4620\n",
      "Saved projector: checkpoints/projector_stage2.pt\n",
      "Saved LoRA adapter: checkpoints/llm_lora\n"
     ]
    }
   ],
   "source": [
    "train_stage2(chat_path, zip_path, projector_ckpt=\"checkpoints/projector_stage1.pt\", max_steps=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z1QaBGvS2625",
    "outputId": "825bd7c4-a8f6-4d32-87c4-3a2d257c7cee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33145"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache(); gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJXsW5bo9DX1"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_once(model, processor, tokenizer, rec):\n",
    "    # build one sample\n",
    "    img_name = rec.get(\"image\") or (rec.get(\"id\",\"\") + \".jpg\")\n",
    "    with zipfile.ZipFile(zip_path,\"r\").open(img_name) as f:\n",
    "        img = Image.open(io.BytesIO(f.read())).convert(\"RGB\")\n",
    "    vis_inputs = processor(images=img, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    # take first human as prompt\n",
    "    user_text = \"Describe the image.\"\n",
    "    for c in rec[\"conversations\"]:\n",
    "        if c[\"from\"].lower()==\"human\":\n",
    "            user_text = c[\"value\"].replace(\"<image>\",\"\").strip() or \"Describe the image.\"\n",
    "            break\n",
    "\n",
    "    user_ids = tokenizer(user_text, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"].to(DEVICE)\n",
    "\n",
    "    # vision\n",
    "    V = model.projector(model.encode_image({k:v for k,v in vis_inputs.items()}))  # (1, Tv, d)\n",
    "    # prepare prefix embeds [V ; user_emb]\n",
    "    user_emb = model.tok_emb(user_ids)  # (1, U, d)\n",
    "    inputs_embeds = torch.cat([V, user_emb], dim=1)\n",
    "    attn_mask = torch.ones(inputs_embeds.size()[:-1], dtype=torch.long, device=inputs_embeds.device)\n",
    "\n",
    "    # generate\n",
    "    gen = model.llm.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attn_mask,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=False\n",
    "    )\n",
    "    # strip the visual prefix length (Tv) + user length (U) when decoding new tokens only\n",
    "    # Easier: decode the full output and print tail\n",
    "    text = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "    print(text)\n",
    "\n",
    "# load for inference\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "m = Stage2Model(\"openai/clip-vit-large-patch14\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"checkpoints/projector_stage2.pt\").to(device)\n",
    "# load LoRA adapter\n",
    "from peft import PeftModel\n",
    "base = m.llm.base_model  # not used directly; we reload via PEFT hub files\n",
    "m.llm.from_pretrained = None  # no-op, avoid confusion\n",
    "m.llm.load_adapter(\"checkpoints/llm_lora\", \"default\")\n",
    "\n",
    "# try on the first record\n",
    "with open(chat_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    recs = json.load(f)\n",
    "generate_once(m, processor, tokenizer, recs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZ4u40mfydaS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
