# LLaVA-Implementation
# LLaVA Mini Replication (Local / Colab)

This project is a **minimal replication** of the paper
**‚ÄúVisual Instruction Tuning‚Äù (LLaVA: Large Language and Vision Assistant)**.

It demonstrates how a multimodal model can learn to connect visual(image) and language(text) understanding in two training stages, using open-source tools only.

------

## üß† Model Architecture

The system combines:

* **Vision Encoder:** `openai/clip-vit-large-patch14` (pretrained CLIP ViT-L/14)
* **Language Model:** `TinyLlama/TinyLlama-1.1B-Chat-v1.0`
* **Projector:** a small linear layer that maps CLIP image features to TinyLlama‚Äôs text embedding space

Architecture flow:

```
Image ‚Üí CLIP Encoder ‚Üí Projector ‚Üí LLM (TinyLlama) ‚Üí Text Response
```

This design follows the same concept as LLaVA but uses a lighter and fully open setup suitable for Colab or local GPUs.

---

## ‚öôÔ∏è Training Overview

### **Stage 1: Feature Alignment**

* Goal: Align CLIP‚Äôs visual embeddings with TinyLlama‚Äôs language space.
* Data: Simple caption-style pairs (e.g., image + short description).
* Process:

  * Freeze CLIP encoder (no fine-tuning).
  * Train only the projector using image‚Äìcaption pairs (`chat.json` format).
  * Result: The projector learns to translate CLIP outputs into TinyLlama‚Äôs embedding space.

### **Stage 2: Visual Instruction Tuning**

* Goal: Make the model follow human-style image‚Äìquestion conversations.
* Data: Instruction-like dialogs (e.g., *‚ÄúDescribe the image‚Äù ‚Üí model response*).
* Process:

  * Use the frozen CLIP + trained projector + TinyLlama (with LoRA adapter).
  * Fine-tune LoRA layers on multimodal conversation data.
  * Result: The model learns multimodal reasoning and natural responses.

------

## üß™ Sanity Check (Evaluation)

After both stages:

1. Load the trained projector and LoRA adapter.
2. Provide a sample from `chat.json`.
3. Run inference:

   ```
   PROMPT: Provide a brief description of the given image.
   GENERATED: olive oil is a healthy ingredient used liberally.
   ```
4. If you see a relevant, coherent output ‚Äî the multimodal alignment is working.


## üìñ Notes

* This is a simplified, educational replication ‚Äî not the full LLaVA scale.
* Inspired by: [LLaVA GitHub](https://github.com/haotian-liu/LLaVA)
* Models used:

  * [TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
  * [CLIP-ViT-L/14](https://huggingface.co/openai/clip-vit-large-patch14)

------

## üèÅ Output Example

| Input Prompt            | Model Output                         |
| ----------------------- | ------------------------------------ |
| "Describe the image."   | "A bowl of fruit on a wooden table." |
| "What object is shown?" | "A green apple."                     |

------



Implementation and experiment by **[Your Name]**,
based on open-source LLaVA methodology and Hugging Face models.
